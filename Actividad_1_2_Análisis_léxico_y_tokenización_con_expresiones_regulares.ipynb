{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZEPaG_2Ffsc",
        "outputId": "1b010a5b-326e-4990-e81e-10070bddde0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b -> identifier\n",
            "= -> op_asign\n",
            "7 -> num_integer\n",
            "-\n",
            "-\n",
            "a -> identifier\n",
            "= -> op_asign\n",
            "32 -> num_integer\n",
            "4 -> num_integer\n",
            "* -> op_multiplication\n",
            "( -> left_paren\n",
            "- -> op_subtraction\n",
            "8 -> num_integer\n",
            "6 -> num_integer\n",
            "- -> op_subtraction\n",
            "b -> identifier\n",
            ") -> right_paren\n",
            "/ -> op_div\n",
            "6 -> num_integer\n",
            "1E-8 -> num_real\n",
            "-\n",
            "-\n",
            "d -> identifier\n",
            "= -> op_asign\n",
            "a -> identifier\n",
            "^ -> op_power\n",
            "b -> identifier\n",
            "/ -> op_div\n",
            "/ -> op_div\n",
            "Esto -> identifier\n",
            "es -> identifier\n",
            "un -> identifier\n",
            "comentario -> identifier\n",
            "-\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def tokenize_line(line, line_number, token_patterns):\n",
        "    tokens = []\n",
        "    position = 0\n",
        "\n",
        "    while position < len(line):\n",
        "        match = None\n",
        "        for token_name, pattern in token_patterns.items():\n",
        "            regex = re.compile(pattern)\n",
        "            match = regex.match(line, position)\n",
        "            if match:\n",
        "                tokens.append((match.group(), token_name))\n",
        "                position = match.end()\n",
        "                break\n",
        "\n",
        "        if not match:\n",
        "            position += 1\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def tokenize_file(filename):\n",
        "    token_patterns = {\n",
        "        \"identifier\": r\"[a-zA-Z_][a-zA-Z0-9_]*\",\n",
        "        \"op_asign\": r\"=\",\n",
        "        \"op_addition\": r\"\\+\",\n",
        "        \"op_subtraction\": r\"-\",\n",
        "        \"op_multiplication\": r\"\\*\",\n",
        "        \"op_div\": r\"/\",\n",
        "        \"op_power\": r\"\\^\",\n",
        "        \"op_less\": r\"<\",\n",
        "        \"op_greater\": r\">\",\n",
        "        \"op_less_equal\": r\"<=\",\n",
        "        \"op_greater_equal\": r\">=\",\n",
        "        \"op_not_equal\": r\"!=\",\n",
        "        \"num_integer\": r\"\\b[0-9]+\\b\",\n",
        "        \"num_float\": r\"\\b[0-9]+\\.[0-9]+\\b\",\n",
        "        \"num_real\": r\"\\b[0-9]+\\.?[0-9]*[eE][-+]?[0-9]+\\b\",\n",
        "        \"string\": r'\".*?\"',\n",
        "        \"keyword\": r\"\\b(for|if|else|while|def|return)\\b\",\n",
        "        \"comment\": r\"//.*\",\n",
        "        \"left_paren\": r\"\\(\",\n",
        "        \"right_paren\": r\"\\)\",\n",
        "        \"left_brace\": r\"\\{\",\n",
        "        \"right_brace\": r\"\\}\",\n",
        "        \"left_bracket\": r\"\\[\",\n",
        "        \"right_bracket\": r\"\\]\",\n",
        "    }\n",
        "\n",
        "    tokenized_lines = []\n",
        "\n",
        "    with open(filename, 'r') as file:\n",
        "        for line_number, line in enumerate(file, start=1):\n",
        "            tokens = tokenize_line(line.strip(), line_number, token_patterns)\n",
        "            tokenized_lines.append(tokens)\n",
        "\n",
        "    return tokenized_lines\n",
        "\n",
        "def main():\n",
        "    filename = \"entrada.txt\"\n",
        "    tokenized_result = tokenize_file(filename)\n",
        "\n",
        "    for line_tokens in tokenized_result:\n",
        "        for token, token_type in line_tokens:\n",
        "            print(f\"{token} -> {token_type}\")\n",
        "        print(\"-\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}